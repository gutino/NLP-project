{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = './Reviews.csv'\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 52268, 2: 29769, 3: 42640, 4: 80655, 5: 363122}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Score'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score                                               Text\n",
      "0      5  I have bought several of the Vitality canned d...\n",
      "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
      "2      4  This is a confection that has been around a fe...\n",
      "3      2  If you are looking for the secret ingredient i...\n",
      "4      5  Great taffy at a great price.  There was a wid...\n"
     ]
    }
   ],
   "source": [
    "del_columns = [col for col in data.columns if col not in ['Text','Score']]\n",
    "data = data.drop(del_columns, 1)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, X_train=None, y_train=None, stem=False):\n",
    "        self.class_docs_count  = dict([]) # número de documentos por classe\n",
    "        self.prior             = dict([]) # proporção do corpus de treinamento\n",
    "        self.likelihood        = dict([]) # chance da palavra ser encontrada na classe\n",
    "        if X_train is not None and y_train is not None:\n",
    "            self.train(X_train, y_train, stem)\n",
    "    \n",
    "    def train(self, X_train, y_train, stem=False):\n",
    "        stemmer = PorterStemmer()         # auxiliares\n",
    "        regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\"\n",
    "        \n",
    "        def to_bow(text):                 # transforma uma string em um dicionário no modelo bag-of-words\n",
    "            words = re.findall(regex, text)\n",
    "            count = {}\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                if word not in count:\n",
    "                    count[word] = 0\n",
    "                count[word] += 1\n",
    "            return count\n",
    "    \n",
    "        def stem_text(text):              # retorna uma string com os radicais das palavras de entrada\n",
    "            words = []\n",
    "            for word in re.findall(regex, text):\n",
    "                words += [stemmer.stem(word)]\n",
    "\n",
    "            return ' '.join(words)\n",
    "        \n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.tolist()\n",
    "        if isinstance(X_train, pd.Series):\n",
    "            X_train = X_train.tolist()\n",
    "            \n",
    "        ''' a partir daqui X e y são listas '''\n",
    "            \n",
    "        if stem == True:\n",
    "            X_train = [stem_text(doc) for doc in X_train]\n",
    "            \n",
    "        self.class_vocabulary = {cl: [] for cl in set(y_train)}\n",
    "        self.vocabulary = set()    \n",
    "        self.class_docs_count = {val: y_train.count(val) for val in set(y_train)}\n",
    "        \n",
    "        # print('class_docs_count =', self.class_docs_count)\n",
    "        \n",
    "        all_docs    = len(X_train)\n",
    "        # print('all docs =', all_docs)\n",
    "        \n",
    "        for text, c in zip(X_train, y_train):\n",
    "            bag = to_bow(text)\n",
    "            for word in bag:\n",
    "                self.vocabulary.add(word)\n",
    "                self.class_vocabulary[c] += [word] * bag[word] # insere n vezes\n",
    "                \n",
    "        # print(self.class_vocabulary)\n",
    "        for c in set(y_train):\n",
    "            class_docs    = self.class_docs_count[c]\n",
    "            self.prior[c] = class_docs/all_docs\n",
    "            count_class_words = len(self.class_vocabulary[c])\n",
    "            # print('count_class_words[%d] ='%(c), count_class_words)\n",
    "            for word in self.vocabulary:\n",
    "                self.likelihood[(word,c)]  = ( (self.class_vocabulary[c].count(word) + 1)/ # suavização de laplace\n",
    "                                               (count_class_words + len(self.vocabulary))  )\n",
    "                # print('lh[%s, %s] = %d + 1/%d + %d'%(word, c, self.class_vocabulary[c].count(word),count_class_words, len(self.vocabulary)))\n",
    "        # print('prior =', self.prior) \n",
    "    def test(self, x_test):\n",
    "        # print('text: ', x_test)\n",
    "        prob = dict([])\n",
    "        for c in self.class_docs_count:\n",
    "            prob[c] = self.prior[c]\n",
    "            for word in re.findall(regex,x_test):\n",
    "                # print('%s ->'%(word), self.likelihood.get((word,c), 1/(len(self.class_vocabulary[c])+len(self.vocabulary))))\n",
    "                prob[c]  *= self.likelihood.get((word,c), 1/(len(self.class_vocabulary[c])+len(self.vocabulary)))\n",
    "        # print('prob', prob)\n",
    "        return max(prob, key=prob.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = ['just plain boring',\n",
    "     'entirely predictable and lacks energy',\n",
    "     'no surprises and very few laughs',\n",
    "     'very powerful',\n",
    "     'the most fun film of the summer']\n",
    "y = ['neg',\n",
    "     'neg',\n",
    "     'neg',\n",
    "     'pos',\n",
    "     'pos']\n",
    "model = NaiveBayes()\n",
    "model.train(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('predictable with no fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 100\n",
    "train_data = data.iloc[:train_size]\n",
    "x = train_data['Text']\n",
    "y = train_data['Score']\n",
    "model.train(x, y)\n",
    "sum([1 for a, b in zip(y, [model.test(val) for val in x]) if a != b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 10\n",
    "test_data = data.iloc[train_size:train_size+test_size]\n",
    "comp = list(zip(test_data['Score'], [model.test(val) for val in test_data['Text']]))\n",
    "diff = sum([1 for a, b in comp if a != b])/test_size\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = data.head(1000)\n",
    "x = test_data['Text']\n",
    "y = test_data['Score']\n",
    "model.train(x, y, stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.604"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_range = 1000\n",
    "comp = list(zip(data['Score'][1000:2000], [model.test(data['Text'][val]) for val in range(1000, 2000)]))\n",
    "diff = sum([1 for a, b in comp if a != b])/test_range\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.339"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp = list(zip(data['Score'][1000:2000], [5 for i in range(1000, 2000)]))\n",
    "diff = sum([1 for a, b in comp if a != b])/test_range\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
